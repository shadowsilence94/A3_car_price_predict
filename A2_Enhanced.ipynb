{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2: Enhanced Linear Regression - Improved Pipeline\n",
    "**Student ID: st126010 - Htut Ko Ko**\n",
    "\n",
    "This notebook implements enhanced linear regression with polynomial features and Lasso regularization using proper data pipeline: split > impute > scale > polynomial > train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Lasso as SklearnLasso\nfrom LinearRegression import LinearRegression, Ridge \n",
    "import mlflow\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('Cars.csv')\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Columns: {list(data.columns)}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data info and missing values\n",
    "print(\"Dataset Info:\")\n",
    "data.info()\n",
    "print(\"\\nMissing values before cleaning:\")\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data - extract numeric values from string columns\n",
    "data['mileage'] = data['mileage'].str.extract('(\\\\d+\\\\.?\\\\d*)').astype(float)\n",
    "data['engine'] = data['engine'].str.extract('(\\\\d+)').astype(float)\n",
    "data['max_power'] = data['max_power'].str.extract('(\\\\d+\\\\.?\\\\d*)').astype(float)\n",
    "\n",
    "# Drop irrelevant columns\n",
    "data = data.drop(columns=['name', 'torque'])\n",
    "\n",
    "print(\"Data cleaning completed\")\n",
    "print(f\"Shape after cleaning: {data.shape}\")\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "numeric_columns = ['year', 'km_driven', 'mileage', 'engine', 'max_power', 'seats']\n",
    "categorical_columns = ['fuel', 'seller_type', 'transmission', 'owner']\n",
    "feature_names = numeric_columns + categorical_columns\n",
    "\n",
    "X = data[feature_names].copy()\n",
    "y = data['selling_price']\n",
    "\n",
    "print(f\"Features: {feature_names}\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"Missing values in features:\")\n",
    "print(X.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Split data FIRST (before any preprocessing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Training missing values: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"Test missing values: {X_test.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Impute missing values (fit on train, transform both)\n",
    "imputer_num = SimpleImputer(strategy='mean')\n",
    "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Impute numeric columns\n",
    "X_train[numeric_columns] = imputer_num.fit_transform(X_train[numeric_columns])\n",
    "X_test[numeric_columns] = imputer_num.transform(X_test[numeric_columns])\n",
    "\n",
    "# Impute categorical columns\n",
    "X_train[categorical_columns] = imputer_cat.fit_transform(X_train[categorical_columns])\n",
    "X_test[categorical_columns] = imputer_cat.transform(X_test[categorical_columns])\n",
    "\n",
    "print(\"✅ Missing values imputed\")\n",
    "print(f\"Training missing values after imputation: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"Test missing values after imputation: {X_test.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Encode categorical variables (fit on train, transform both)\n",
    "label_encoders = {}\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    X_train[col] = le.fit_transform(X_train[col].astype(str))\n",
    "    X_test[col] = le.transform(X_test[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(\"✅ Categorical variables encoded\")\n",
    "print(f\"Label encoders created for: {list(label_encoders.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Scale features (fit on train, transform both)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"✅ Features scaled\")\n",
    "print(f\"Scaled training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled test set shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Create polynomial features (fit on train, transform both)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "X_test_poly = poly.transform(X_test_scaled)\n",
    "\n",
    "print(\"✅ Polynomial features created\")\n",
    "print(f\"Polynomial training set shape: {X_train_poly.shape}\")\n",
    "print(f\"Polynomial test set shape: {X_test_poly.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation function\n",
    "def cross_validate_model(model, X_data, y_data, cv_folds=5):\n",
    "    \"\"\"\n",
    "    Perform cross-validation for a given instantiated model.\n",
    "    \"\"\"\n",
    "    kfold = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "    r2_scores = []\n",
    "    mse_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_data)):\n",
    "        # Split data for this fold\n",
    "        X_fold_train, X_fold_val = X_data[train_idx], X_data[val_idx]\n",
    "        y_fold_train, y_fold_val = y_data.iloc[train_idx], y_data.iloc[val_idx]\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_fold_train, y_fold_val.values)\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        y_pred = model.predict(X_fold_val)\n",
    "        r2 = model.r2(X_fold_val, y_fold_val.values)\n",
    "        mse = mean_squared_error(y_fold_val.values, y_pred)\n",
    "        \n",
    "        r2_scores.append(r2)\n",
    "        mse_scores.append(mse)\n",
    "    \n",
    "    return {\n",
    "        'mean_r2': np.mean(r2_scores),\n",
    "        'std_r2': np.std(r2_scores),\n",
    "        'mean_mse': np.mean(mse_scores),\n",
    "        'std_mse': np.std(mse_scores),\n",
    "    }\n",
    "\n",
    "print(\"Cross-validation function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment configurations\n",
    "experiments = {\n",
    "    'linear_polynomial': {\n",
    "        'model': LinearRegression(),\n",
    "        'data': X_train_poly,\n",
    "        'description': 'Linear Regression with Polynomial Features'\n",
    "    },\n",
    "    'ridge_polynomial': {\n",
    "        'model': Ridge(alpha=0.1),\n",
    "        'data': X_train_poly,\n",
    "        'description': 'Ridge Regression with Polynomial Features'\n",
    "    },\n",
    "    'lasso_polynomial': {\n",
    "        'model': SklearnLasso(alpha=0.1, random_state=42),\n",
    "        'data': X_train_poly,\n",
    "        'description': 'Lasso Regression with Polynomial Features'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Experiment configurations defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments with cross-validation\n",
    "results = []\n",
    "best_model = None\n",
    "best_score = -np.inf\n",
    "\n",
    "print(\"Running experiments with cross-validation...\\n\")\n",
    "\n",
    "for exp_name, config in experiments.items():\n",
    "    print(f\"Running {exp_name}: {config['description']}\")\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = cross_validate_model(config['model'], config['data'], y_train)\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        'experiment_name': exp_name,\n",
    "        'model_type': type(config['model']).__name__,\n",
    "        'mean_r2': cv_results['mean_r2'],\n",
    "        'std_r2': cv_results['std_r2'],\n",
    "        'mean_mse': cv_results['mean_mse'],\n",
    "        'std_mse': cv_results['std_mse']\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    # Track best model\n",
    "    if cv_results['mean_r2'] > best_score:\n",
    "        best_score = cv_results['mean_r2']\n",
    "        best_model = config\n",
    "        best_model['name'] = exp_name\n",
    "    \n",
    "    print(f\"  Mean R²: {cv_results['mean_r2']:.4f} (±{cv_results['std_r2']:.4f})\")\n",
    "    print(f\"  Mean MSE: {cv_results['mean_mse']:.2f} (±{cv_results['std_mse']:.2f})\\n\")\n",
    "\n",
    "print(f\"Best model: {best_model['name']} with R² = {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the best model on the test set\n",
    "if best_model:\n",
    "    print(\"Training Best Model on Full Dataset\")\n",
    "    print(f\"Best Configuration: {best_model['name']}\")\n",
    "    \n",
    "    # Train on full training set\n",
    "    final_model = best_model['model']\n",
    "    final_model.fit(best_model['data'], y_train.values)\n",
    "    \n",
    "    # Predict on test set\n",
    "    if best_model['name'] == 'lasso_polynomial':\n",
    "        y_pred_test = final_model.predict(X_test_poly)\n",
    "        test_r2 = final_model.r2(X_test_poly, y_test.values)\n",
    "    else:\n",
    "        y_pred_test = final_model.predict(X_test_poly)\n",
    "        test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "    \n",
    "    print(f\"\\nFinal Test Results:\")\n",
    "    print(f\"- Test R2 Score: {test_r2:.4f}\")\n",
    "    print(f\"- Test MSE: {test_mse:.2f}\")\n",
    "    \n",
    "    # Save the model and all necessary artifacts\n",
    "    model_artifacts = {\n",
    "        'model': final_model,\n",
    "        'scaler': scaler,\n",
    "        'poly': poly,\n",
    "        'imputer_num': imputer_num,\n",
    "        'imputer_cat': imputer_cat,\n",
    "        'label_encoders': label_encoders,\n",
    "        'feature_names': feature_names,\n",
    "        'metrics': {\n",
    "            'test_r2': test_r2,\n",
    "            'test_mse': test_mse,\n",
    "            'cv_mean_r2': best_score\n",
    "        },\n",
    "        'model_type': best_model['name']\n",
    "    }\n",
    "    \n",
    "    with open('a2_model_artifacts.pkl', 'wb') as f:\n",
    "        pickle.dump(model_artifacts, f)\n",
    "    \n",
    "    print(\"\\nModel saved as 'a2_model_artifacts.pkl'\")\n",
    "    print(f\"🎯 Final Test R²: {test_r2:.4f}\")\n",
    "    print(\"🔄 Pipeline: Load → Clean → Split → Impute → Scale → Polynomial → Train → Evaluate\")\n",
    "else:\n",
    "    print(\"No experiments were run successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "if best_model and 'y_pred_test' in locals():\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(y_test, y_pred_test, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual Price')\n",
    "    plt.ylabel('Predicted Price')\n",
    "    plt.title(f'A2: Actual vs Predicted (R² = {test_r2:.4f})')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    residuals = y_test - y_pred_test\n",
    "    plt.scatter(y_pred_test, residuals, alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('Predicted Price')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residual Plot')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display results summary\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\nExperiment Results Summary:\")\n",
    "    print(results_df[['experiment_name', 'model_type', 'mean_r2', 'std_r2']].round(4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

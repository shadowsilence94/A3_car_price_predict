{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A1: Linear Regression - Car Price Prediction\n",
    "**Student ID: st126010 - Htut Ko Ko**\n",
    "\n",
    "This notebook implements basic linear regression for car price prediction following the assignment requirements with proper ML pipeline to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('Cars.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nDataset info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Data Preprocessing (Following Assignment Requirements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for preprocessing\n",
    "data = df.copy()\n",
    "print(f\"Original dataset shape: {data.shape}\")\n",
    "\n",
    "# 1. Remove CNG and LPG rows (different mileage system)\n",
    "print(f\"\\nFuel types before filtering: {data['fuel'].value_counts()}\")\n",
    "data = data[~data['fuel'].isin(['CNG', 'LPG'])]\n",
    "print(f\"Fuel types after filtering: {data['fuel'].value_counts()}\")\n",
    "print(f\"Shape after removing CNG/LPG: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Remove Test Drive Cars (ridiculously expensive)\n",
    "print(f\"\\nOwner types before filtering: {data['owner'].value_counts()}\")\n",
    "data = data[data['owner'] != 'Test Drive Car']\n",
    "print(f\"Owner types after filtering: {data['owner'].value_counts()}\")\n",
    "print(f\"Shape after removing Test Drive Cars: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Map owner feature: First Owner=1, Second Owner=2, etc.\n",
    "owner_mapping = {\n",
    "    'First Owner': 1,\n",
    "    'Second Owner': 2, \n",
    "    'Third Owner': 3,\n",
    "    'Fourth & Above Owner': 4\n",
    "}\n",
    "data['owner'] = data['owner'].map(owner_mapping)\n",
    "print(f\"\\nOwner mapping applied: {data['owner'].value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Clean mileage column - remove 'kmpl' and convert to float\n",
    "print(f\"\\nMileage before cleaning (sample): {data['mileage'].head()}\")\n",
    "data['mileage'] = data['mileage'].str.split().str[0]  # Take first part before space\n",
    "data['mileage'] = pd.to_numeric(data['mileage'], errors='coerce')\n",
    "print(f\"Mileage after cleaning (sample): {data['mileage'].head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Clean engine column - remove 'CC' and convert to float\n",
    "print(f\"\\nEngine before cleaning (sample): {data['engine'].head()}\")\n",
    "data['engine'] = data['engine'].str.replace(' CC', '').str.replace('CC', '')\n",
    "data['engine'] = pd.to_numeric(data['engine'], errors='coerce')\n",
    "print(f\"Engine after cleaning (sample): {data['engine'].head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Clean max_power column - remove 'bhp' and convert to float\n",
    "print(f\"\\nMax power before cleaning (sample): {data['max_power'].head()}\")\n",
    "data['max_power'] = data['max_power'].str.replace(' bhp', '').str.replace('bhp', '')\n",
    "data['max_power'] = pd.to_numeric(data['max_power'], errors='coerce')\n",
    "print(f\"Max power after cleaning (sample): {data['max_power'].head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Extract brand from name (first word only)\n",
    "print(f\"\\nName before brand extraction (sample): {data['name'].head()}\")\n",
    "data['brand'] = data['name'].str.split().str[0]\n",
    "print(f\"Brand after extraction (sample): {data['brand'].head()}\")\n",
    "print(f\"Unique brands: {data['brand'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Drop torque column (as per assignment requirement)\n",
    "if 'torque' in data.columns:\n",
    "    data = data.drop('torque', axis=1)\n",
    "    print(\"Torque column dropped\")\n",
    "\n",
    "# Also drop name column since we extracted brand\n",
    "data = data.drop('name', axis=1)\n",
    "print(\"Name column dropped (brand extracted)\")\n",
    "\n",
    "print(f\"\\nFinal columns: {list(data.columns)}\")\n",
    "print(f\"Final shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Proper ML Pipeline - Split â†’ Impute â†’ Scale (Prevent Data Leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Prepare features and target (NO PREPROCESSING YET)\n",
    "# Apply log transformation to target variable (as per assignment requirement)\n",
    "y = np.log(data['selling_price'])\n",
    "X = data.drop('selling_price', axis=1)\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeatures: {list(X.columns)}\")\n",
    "print(f\"\\nMissing values before split:\")\n",
    "print(X.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: SPLIT FIRST (before any preprocessing to prevent data leakage)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(\"\\nâœ… Data split BEFORE preprocessing - no data leakage!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: IMPUTE missing values (fit on train, transform both)\n",
    "# Handle missing numerical values\n",
    "numerical_cols = ['mileage', 'engine', 'max_power']\n",
    "imputer_num = SimpleImputer(strategy='median')\n",
    "\n",
    "# Create copies to avoid modifying original data\n",
    "X_train_processed = X_train.copy()\n",
    "X_test_processed = X_test.copy()\n",
    "\n",
    "# Fit imputer on training data only, transform both\n",
    "X_train_processed[numerical_cols] = imputer_num.fit_transform(X_train_processed[numerical_cols])\n",
    "X_test_processed[numerical_cols] = imputer_num.transform(X_test_processed[numerical_cols])\n",
    "\n",
    "print(\"âœ… Imputation completed - fit on train, transform on both\")\n",
    "print(f\"Training missing values: {X_train_processed[numerical_cols].isnull().sum().sum()}\")\n",
    "print(f\"Test missing values: {X_test_processed[numerical_cols].isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: ENCODE categorical variables (fit on train, transform both)\n",
    "label_encoders = {}\n",
    "categorical_cols = ['fuel', 'seller_type', 'transmission', 'brand']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    # Fit on training data only\n",
    "    X_train_processed[col] = le.fit_transform(X_train_processed[col])\n",
    "    # Transform test data using training encoder\n",
    "    X_test_processed[col] = le.transform(X_test_processed[col])\n",
    "    label_encoders[col] = le\n",
    "    print(f\"Encoded {col}: {len(le.classes_)} unique values\")\n",
    "\n",
    "print(f\"\\nâœ… Label encoders fit on training data only - no data leakage!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: SCALE features (fit on train, transform both)\n",
    "scaler = StandardScaler()\n",
    "# Fit scaler on training data only\n",
    "X_train_scaled = scaler.fit_transform(X_train_processed)\n",
    "# Transform test data using training statistics\n",
    "X_test_scaled = scaler.transform(X_test_processed)\n",
    "\n",
    "print(\"âœ… Scaling completed - fit on train, transform on both\")\n",
    "print(f\"Training features mean: {X_train_scaled.mean():.6f}\")\n",
    "print(f\"Training features std: {X_train_scaled.std():.6f}\")\n",
    "print(f\"Test features mean: {X_test_scaled.mean():.6f}\")\n",
    "print(f\"Test features std: {X_test_scaled.std():.6f}\")\n",
    "print(\"\\nðŸŽ¯ PROPER ML PIPELINE: Split â†’ Impute â†’ Encode â†’ Scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Linear Regression model trained successfully\")\n",
    "print(f\"Model coefficients shape: {model.coef_.shape}\")\n",
    "print(f\"Model intercept: {model.intercept_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions (log scale)\n",
    "y_train_pred_log = model.predict(X_train_scaled)\n",
    "y_test_pred_log = model.predict(X_test_scaled)\n",
    "\n",
    "# Transform back to original scale (as per assignment requirement)\n",
    "y_train_pred = np.exp(y_train_pred_log)\n",
    "y_test_pred = np.exp(y_test_pred_log)\n",
    "y_train_actual = np.exp(y_train)\n",
    "y_test_actual = np.exp(y_test)\n",
    "\n",
    "print(\"Predictions completed and transformed back to original scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "train_r2 = r2_score(y_train_actual, y_train_pred)\n",
    "test_r2 = r2_score(y_test_actual, y_test_pred)\n",
    "train_mse = mean_squared_error(y_train_actual, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test_actual, y_test_pred)\n",
    "\n",
    "print(\"=== Model Performance ===\")\n",
    "print(f\"Training RÂ²: {train_r2:.4f}\")\n",
    "print(f\"Test RÂ²: {test_r2:.4f}\")\n",
    "print(f\"Training MSE: {train_mse:,.0f}\")\n",
    "print(f\"Test MSE: {test_mse:,.0f}\")\n",
    "print(f\"Training RMSE: {np.sqrt(train_mse):,.0f}\")\n",
    "print(f\"Test RMSE: {np.sqrt(test_mse):,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train_processed.columns,\n",
    "    'coefficient': model.coef_,\n",
    "    'abs_coefficient': np.abs(model.coef_)\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (by coefficient magnitude):\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Actual vs Predicted\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(y_test_actual, y_test_pred, alpha=0.6)\n",
    "plt.plot([y_test_actual.min(), y_test_actual.max()], [y_test_actual.min(), y_test_actual.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title(f'Actual vs Predicted (RÂ² = {test_r2:.4f})')\n",
    "\n",
    "# Residuals\n",
    "plt.subplot(1, 3, 2)\n",
    "residuals = y_test_actual - y_test_pred\n",
    "plt.scatter(y_test_pred, residuals, alpha=0.6)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Price')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "\n",
    "# Feature importance\n",
    "plt.subplot(1, 3, 3)\n",
    "top_features = feature_importance.head(8)\n",
    "plt.barh(range(len(top_features)), top_features['abs_coefficient'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Absolute Coefficient')\n",
    "plt.title('Top Feature Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and preprocessing components\n",
    "model_artifacts = {\n",
    "    'model': model,\n",
    "    'scaler': scaler,\n",
    "    'imputer_num': imputer_num,\n",
    "    'label_encoders': label_encoders,\n",
    "    'feature_names': list(X_train_processed.columns),\n",
    "    'numerical_cols': numerical_cols,\n",
    "    'categorical_cols': categorical_cols,\n",
    "    'metrics': {\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'train_mse': train_mse,\n",
    "        'test_mse': test_mse\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('a1_model_artifacts.pkl', 'wb') as f:\n",
    "    pickle.dump(model_artifacts, f)\n",
    "\n",
    "print(\"Model artifacts saved to 'a1_model_artifacts.pkl'\")\n",
    "print(f\"Saved components: {list(model_artifacts.keys())}\")\n",
    "print(\"\\nâœ… All preprocessing components saved - ready for deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Analysis and Discussion\n",
    "\n",
    "### Results Analysis\n",
    "\n",
    "**Model Performance:**\n",
    "The Linear Regression model achieved a test RÂ² score of {test_r2:.4f}, indicating that the model explains approximately {test_r2*100:.1f}% of the variance in car prices. This represents a solid baseline performance for a simple linear model with proper data preprocessing pipeline.\n",
    "\n",
    "**Proper ML Pipeline Implementation:**\n",
    "This implementation follows the critical **Split â†’ Impute â†’ Encode â†’ Scale** pipeline to prevent data leakage:\n",
    "1. **Split First**: Data was split before any preprocessing to ensure test data remains unseen\n",
    "2. **Fit on Train Only**: All preprocessing (imputation, encoding, scaling) was fitted only on training data\n",
    "3. **Transform Both**: Test data was transformed using training statistics, preventing information leakage\n",
    "4. **Proper Validation**: This ensures realistic performance estimates that generalize to new data\n",
    "\n",
    "**Feature Importance:**\n",
    "Based on the coefficient analysis, the most important features for predicting car prices are:\n",
    "1. **Year**: Newer cars command higher prices due to depreciation patterns\n",
    "2. **Engine Size**: Larger engines typically indicate more powerful and expensive vehicles\n",
    "3. **Max Power**: Higher power output directly correlates with premium pricing\n",
    "4. **Brand**: Certain brands maintain premium positioning and resale value\n",
    "5. **Mileage**: Fuel efficiency impacts pricing differently across market segments\n",
    "\n",
    "**Data Preprocessing Impact:**\n",
    "The log transformation of the target variable was essential for handling the wide price range and skewed distribution. Removing CNG/LPG vehicles and Test Drive Cars helped focus on consistent market segments. The proper handling of missing values through median imputation preserved data integrity while maintaining realistic distributions.\n",
    "\n",
    "**Model Limitations and Future Improvements:**\n",
    "While this linear model provides a solid baseline, it assumes linear relationships between features and log-price. The residual analysis suggests some heteroscedasticity, indicating that more sophisticated models (polynomial features, regularization, or ensemble methods) might capture additional patterns. The current approach treats all brands equally through label encoding, but brand hierarchy and premium positioning could be better captured through more sophisticated encoding strategies.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
